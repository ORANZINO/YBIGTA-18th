{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_generation_using_RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zawKRM6f-sJ4"},"source":["1) 데이터에 대한 이해와 전처리"]},{"cell_type":"code","metadata":{"id":"zFOpIP-5g394"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47z91O0XhVRK"},"source":["# 예제 문장 저장\n","\n","text = \"\"\"경마장에 있는 말이 뛰고 있다\\n그의 말이 법이다\\n가는 말이 고와야 오는 말이 곱다\\n\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qN3ILIashYuh","outputId":"455ee145-a641-44f3-b56d-f4a212416d2a"},"source":["t= Tokenizer()\n","t.fit_on_texts([text])\n","vocab_size = len(t.word_index) + 1\n","\n","# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n","# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n","# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야함\n","\n","print('단어 집합의 크기: {}'.format(vocab_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["단어 집합의 크기: 12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp9iHXVthrcV","outputId":"10e7c885-6c6c-4257-c3c6-04d2823d7a8d"},"source":["print(t.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JdWdL-CIyj6-"},"source":["# 훈련 데이터 만들기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jz8uZMGh4k_","outputId":"2fd34149-3089-46c7-c124-955044d29505"},"source":["sequences = []\n","for line in text.split('\\n'):    # \\n 기준으로 문장 토큰화\n","    encoded = t.texts_to_sequences([line])[0]\n","    for i in range(1, len(encoded)):\n","        sequence = encoded[:i+1]   # 길이가 2 이상인 gram들 저장\n","        sequences.append(sequence)\n","\n","print('학습에 사용할 샘플의 개수: {}'.format(len(sequences)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["학습에 사용할 샘플의 개수: 11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mHWTK0R_5Wf","outputId":"1bb48274-04c3-44cf-a7f2-c5650f283f86"},"source":["for i in sequences:\n","    print(i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2, 3]\n","[2, 3, 1]\n","[2, 3, 1, 4]\n","[2, 3, 1, 4, 5]\n","[6, 1]\n","[6, 1, 7]\n","[8, 1]\n","[8, 1, 9]\n","[8, 1, 9, 10]\n","[8, 1, 9, 10, 1]\n","[8, 1, 9, 10, 1, 11]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0MB72Aj5aUk","outputId":"04a251b4-5dbe-4a1c-ce79-f94839bfcf29"},"source":["# 가장 긴 샘플의 길이에 전체 샘플 길이 맞추기\n","\n","max_len = max(len(l) for l in sequences) \n","print('샘플 최대 길이 : {}'.format(max_len))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["샘플 최대 길이 : 6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zleVObL85r7C","outputId":"e5a6e23d-3648-4d36-a10f-dc46d8d24356"},"source":["# 전체 샘플 길이를 6으로 패딩\n","\n","sequences = pad_sequences(sequences, maxlen = max_len, padding='pre')\n","print(sequences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0  0  0  0  2  3]\n"," [ 0  0  0  2  3  1]\n"," [ 0  0  2  3  1  4]\n"," [ 0  2  3  1  4  5]\n"," [ 0  0  0  0  6  1]\n"," [ 0  0  0  6  1  7]\n"," [ 0  0  0  0  8  1]\n"," [ 0  0  0  8  1  9]\n"," [ 0  0  8  1  9 10]\n"," [ 0  8  1  9 10  1]\n"," [ 8  1  9 10  1 11]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zyiyvUTG7s5e"},"source":["# 각 샘플의 마지막 단어 레이블로 분리하기\n","\n","sequences = np.array(sequences)\n","\n","X = sequences[:,:-1]      # 각 리스트의 마지막 값을 제외하고 저장\n","y = sequences[:,-1]       # 각 리스트의 마지막 값만 저장"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0J5ddD39EqY","outputId":"60c1d636-72dc-4b0e-8185-88f198422753"},"source":["print(X)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0  0  0  0  2]\n"," [ 0  0  0  2  3]\n"," [ 0  0  2  3  1]\n"," [ 0  2  3  1  4]\n"," [ 0  0  0  0  6]\n"," [ 0  0  0  6  1]\n"," [ 0  0  0  0  8]\n"," [ 0  0  0  8  1]\n"," [ 0  0  8  1  9]\n"," [ 0  8  1  9 10]\n"," [ 8  1  9 10  1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwiMCMBBAlMO","outputId":"f9dc78bf-52cb-42c4-a0c0-a9d4d6949f09"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 3  1  4  5  1  7  1  9 10  1 11]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6nZvP5w39g7D"},"source":["# 레이블에 대해 one-hot encoding\n","y = to_categorical(y, num_classes = vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Srwt_wAd-c0d","outputId":"fdcd99c4-fddc-4700-90a5-0926bc958658"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7UaI8AmV-m85"},"source":["2) 모델 설계하기"]},{"cell_type":"code","metadata":{"id":"323ioVIK-ex2"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7MPtrNk-8nn","outputId":"14418ccd-82a4-43b2-d364-28ac6bfd57d6"},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 10, input_length = max_len-1)) # 각 단어를 10차원으로 embedding, 레이블 분리 후 길이는 -1\n","model.add(SimpleRNN(32))   # 은닉 상태 크기\n","model.add(Dense(vocab_size, activation = 'softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","model.fit(X,y,epochs = 200, verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","1/1 - 1s - loss: 2.4973 - accuracy: 0.0909\n","Epoch 2/200\n","1/1 - 0s - loss: 2.4844 - accuracy: 0.1818\n","Epoch 3/200\n","1/1 - 0s - loss: 2.4715 - accuracy: 0.0909\n","Epoch 4/200\n","1/1 - 0s - loss: 2.4587 - accuracy: 0.1818\n","Epoch 5/200\n","1/1 - 0s - loss: 2.4457 - accuracy: 0.1818\n","Epoch 6/200\n","1/1 - 0s - loss: 2.4325 - accuracy: 0.2727\n","Epoch 7/200\n","1/1 - 0s - loss: 2.4189 - accuracy: 0.2727\n","Epoch 8/200\n","1/1 - 0s - loss: 2.4049 - accuracy: 0.2727\n","Epoch 9/200\n","1/1 - 0s - loss: 2.3905 - accuracy: 0.3636\n","Epoch 10/200\n","1/1 - 0s - loss: 2.3754 - accuracy: 0.3636\n","Epoch 11/200\n","1/1 - 0s - loss: 2.3596 - accuracy: 0.3636\n","Epoch 12/200\n","1/1 - 0s - loss: 2.3430 - accuracy: 0.3636\n","Epoch 13/200\n","1/1 - 0s - loss: 2.3256 - accuracy: 0.3636\n","Epoch 14/200\n","1/1 - 0s - loss: 2.3073 - accuracy: 0.3636\n","Epoch 15/200\n","1/1 - 0s - loss: 2.2880 - accuracy: 0.3636\n","Epoch 16/200\n","1/1 - 0s - loss: 2.2676 - accuracy: 0.3636\n","Epoch 17/200\n","1/1 - 0s - loss: 2.2463 - accuracy: 0.3636\n","Epoch 18/200\n","1/1 - 0s - loss: 2.2239 - accuracy: 0.3636\n","Epoch 19/200\n","1/1 - 0s - loss: 2.2006 - accuracy: 0.3636\n","Epoch 20/200\n","1/1 - 0s - loss: 2.1763 - accuracy: 0.3636\n","Epoch 21/200\n","1/1 - 0s - loss: 2.1513 - accuracy: 0.3636\n","Epoch 22/200\n","1/1 - 0s - loss: 2.1256 - accuracy: 0.3636\n","Epoch 23/200\n","1/1 - 0s - loss: 2.0997 - accuracy: 0.3636\n","Epoch 24/200\n","1/1 - 0s - loss: 2.0737 - accuracy: 0.3636\n","Epoch 25/200\n","1/1 - 0s - loss: 2.0480 - accuracy: 0.3636\n","Epoch 26/200\n","1/1 - 0s - loss: 2.0231 - accuracy: 0.3636\n","Epoch 27/200\n","1/1 - 0s - loss: 1.9995 - accuracy: 0.3636\n","Epoch 28/200\n","1/1 - 0s - loss: 1.9775 - accuracy: 0.3636\n","Epoch 29/200\n","1/1 - 0s - loss: 1.9577 - accuracy: 0.3636\n","Epoch 30/200\n","1/1 - 0s - loss: 1.9402 - accuracy: 0.3636\n","Epoch 31/200\n","1/1 - 0s - loss: 1.9250 - accuracy: 0.3636\n","Epoch 32/200\n","1/1 - 0s - loss: 1.9118 - accuracy: 0.3636\n","Epoch 33/200\n","1/1 - 0s - loss: 1.9001 - accuracy: 0.3636\n","Epoch 34/200\n","1/1 - 0s - loss: 1.8891 - accuracy: 0.3636\n","Epoch 35/200\n","1/1 - 0s - loss: 1.8781 - accuracy: 0.3636\n","Epoch 36/200\n","1/1 - 0s - loss: 1.8666 - accuracy: 0.3636\n","Epoch 37/200\n","1/1 - 0s - loss: 1.8542 - accuracy: 0.3636\n","Epoch 38/200\n","1/1 - 0s - loss: 1.8409 - accuracy: 0.3636\n","Epoch 39/200\n","1/1 - 0s - loss: 1.8269 - accuracy: 0.3636\n","Epoch 40/200\n","1/1 - 0s - loss: 1.8124 - accuracy: 0.3636\n","Epoch 41/200\n","1/1 - 0s - loss: 1.7977 - accuracy: 0.3636\n","Epoch 42/200\n","1/1 - 0s - loss: 1.7831 - accuracy: 0.3636\n","Epoch 43/200\n","1/1 - 0s - loss: 1.7688 - accuracy: 0.3636\n","Epoch 44/200\n","1/1 - 0s - loss: 1.7550 - accuracy: 0.3636\n","Epoch 45/200\n","1/1 - 0s - loss: 1.7416 - accuracy: 0.3636\n","Epoch 46/200\n","1/1 - 0s - loss: 1.7285 - accuracy: 0.4545\n","Epoch 47/200\n","1/1 - 0s - loss: 1.7156 - accuracy: 0.4545\n","Epoch 48/200\n","1/1 - 0s - loss: 1.7026 - accuracy: 0.4545\n","Epoch 49/200\n","1/1 - 0s - loss: 1.6895 - accuracy: 0.4545\n","Epoch 50/200\n","1/1 - 0s - loss: 1.6761 - accuracy: 0.4545\n","Epoch 51/200\n","1/1 - 0s - loss: 1.6625 - accuracy: 0.4545\n","Epoch 52/200\n","1/1 - 0s - loss: 1.6484 - accuracy: 0.4545\n","Epoch 53/200\n","1/1 - 0s - loss: 1.6341 - accuracy: 0.4545\n","Epoch 54/200\n","1/1 - 0s - loss: 1.6195 - accuracy: 0.4545\n","Epoch 55/200\n","1/1 - 0s - loss: 1.6047 - accuracy: 0.4545\n","Epoch 56/200\n","1/1 - 0s - loss: 1.5897 - accuracy: 0.4545\n","Epoch 57/200\n","1/1 - 0s - loss: 1.5745 - accuracy: 0.4545\n","Epoch 58/200\n","1/1 - 0s - loss: 1.5592 - accuracy: 0.4545\n","Epoch 59/200\n","1/1 - 0s - loss: 1.5437 - accuracy: 0.4545\n","Epoch 60/200\n","1/1 - 0s - loss: 1.5281 - accuracy: 0.4545\n","Epoch 61/200\n","1/1 - 0s - loss: 1.5122 - accuracy: 0.4545\n","Epoch 62/200\n","1/1 - 0s - loss: 1.4961 - accuracy: 0.4545\n","Epoch 63/200\n","1/1 - 0s - loss: 1.4799 - accuracy: 0.4545\n","Epoch 64/200\n","1/1 - 0s - loss: 1.4634 - accuracy: 0.4545\n","Epoch 65/200\n","1/1 - 0s - loss: 1.4468 - accuracy: 0.4545\n","Epoch 66/200\n","1/1 - 0s - loss: 1.4301 - accuracy: 0.5455\n","Epoch 67/200\n","1/1 - 0s - loss: 1.4134 - accuracy: 0.5455\n","Epoch 68/200\n","1/1 - 0s - loss: 1.3966 - accuracy: 0.5455\n","Epoch 69/200\n","1/1 - 0s - loss: 1.3798 - accuracy: 0.5455\n","Epoch 70/200\n","1/1 - 0s - loss: 1.3631 - accuracy: 0.5455\n","Epoch 71/200\n","1/1 - 0s - loss: 1.3464 - accuracy: 0.5455\n","Epoch 72/200\n","1/1 - 0s - loss: 1.3297 - accuracy: 0.5455\n","Epoch 73/200\n","1/1 - 0s - loss: 1.3131 - accuracy: 0.5455\n","Epoch 74/200\n","1/1 - 0s - loss: 1.2964 - accuracy: 0.5455\n","Epoch 75/200\n","1/1 - 0s - loss: 1.2798 - accuracy: 0.5455\n","Epoch 76/200\n","1/1 - 0s - loss: 1.2632 - accuracy: 0.5455\n","Epoch 77/200\n","1/1 - 0s - loss: 1.2466 - accuracy: 0.6364\n","Epoch 78/200\n","1/1 - 0s - loss: 1.2300 - accuracy: 0.6364\n","Epoch 79/200\n","1/1 - 0s - loss: 1.2134 - accuracy: 0.6364\n","Epoch 80/200\n","1/1 - 0s - loss: 1.1969 - accuracy: 0.6364\n","Epoch 81/200\n","1/1 - 0s - loss: 1.1803 - accuracy: 0.6364\n","Epoch 82/200\n","1/1 - 0s - loss: 1.1637 - accuracy: 0.6364\n","Epoch 83/200\n","1/1 - 0s - loss: 1.1471 - accuracy: 0.6364\n","Epoch 84/200\n","1/1 - 0s - loss: 1.1306 - accuracy: 0.6364\n","Epoch 85/200\n","1/1 - 0s - loss: 1.1140 - accuracy: 0.6364\n","Epoch 86/200\n","1/1 - 0s - loss: 1.0975 - accuracy: 0.6364\n","Epoch 87/200\n","1/1 - 0s - loss: 1.0810 - accuracy: 0.6364\n","Epoch 88/200\n","1/1 - 0s - loss: 1.0644 - accuracy: 0.6364\n","Epoch 89/200\n","1/1 - 0s - loss: 1.0479 - accuracy: 0.6364\n","Epoch 90/200\n","1/1 - 0s - loss: 1.0314 - accuracy: 0.7273\n","Epoch 91/200\n","1/1 - 0s - loss: 1.0149 - accuracy: 0.7273\n","Epoch 92/200\n","1/1 - 0s - loss: 0.9985 - accuracy: 0.7273\n","Epoch 93/200\n","1/1 - 0s - loss: 0.9821 - accuracy: 0.7273\n","Epoch 94/200\n","1/1 - 0s - loss: 0.9657 - accuracy: 0.7273\n","Epoch 95/200\n","1/1 - 0s - loss: 0.9495 - accuracy: 0.8182\n","Epoch 96/200\n","1/1 - 0s - loss: 0.9333 - accuracy: 0.8182\n","Epoch 97/200\n","1/1 - 0s - loss: 0.9171 - accuracy: 0.8182\n","Epoch 98/200\n","1/1 - 0s - loss: 0.9011 - accuracy: 0.8182\n","Epoch 99/200\n","1/1 - 0s - loss: 0.8852 - accuracy: 0.8182\n","Epoch 100/200\n","1/1 - 0s - loss: 0.8694 - accuracy: 0.8182\n","Epoch 101/200\n","1/1 - 0s - loss: 0.8537 - accuracy: 0.8182\n","Epoch 102/200\n","1/1 - 0s - loss: 0.8381 - accuracy: 0.8182\n","Epoch 103/200\n","1/1 - 0s - loss: 0.8227 - accuracy: 0.8182\n","Epoch 104/200\n","1/1 - 0s - loss: 0.8074 - accuracy: 0.8182\n","Epoch 105/200\n","1/1 - 0s - loss: 0.7923 - accuracy: 0.8182\n","Epoch 106/200\n","1/1 - 0s - loss: 0.7774 - accuracy: 0.8182\n","Epoch 107/200\n","1/1 - 0s - loss: 0.7626 - accuracy: 0.9091\n","Epoch 108/200\n","1/1 - 0s - loss: 0.7481 - accuracy: 0.9091\n","Epoch 109/200\n","1/1 - 0s - loss: 0.7337 - accuracy: 0.9091\n","Epoch 110/200\n","1/1 - 0s - loss: 0.7195 - accuracy: 0.9091\n","Epoch 111/200\n","1/1 - 0s - loss: 0.7056 - accuracy: 0.9091\n","Epoch 112/200\n","1/1 - 0s - loss: 0.6918 - accuracy: 0.9091\n","Epoch 113/200\n","1/1 - 0s - loss: 0.6783 - accuracy: 0.9091\n","Epoch 114/200\n","1/1 - 0s - loss: 0.6649 - accuracy: 0.9091\n","Epoch 115/200\n","1/1 - 0s - loss: 0.6518 - accuracy: 0.9091\n","Epoch 116/200\n","1/1 - 0s - loss: 0.6389 - accuracy: 0.9091\n","Epoch 117/200\n","1/1 - 0s - loss: 0.6263 - accuracy: 0.9091\n","Epoch 118/200\n","1/1 - 0s - loss: 0.6138 - accuracy: 0.9091\n","Epoch 119/200\n","1/1 - 0s - loss: 0.6016 - accuracy: 0.9091\n","Epoch 120/200\n","1/1 - 0s - loss: 0.5896 - accuracy: 0.9091\n","Epoch 121/200\n","1/1 - 0s - loss: 0.5778 - accuracy: 0.9091\n","Epoch 122/200\n","1/1 - 0s - loss: 0.5663 - accuracy: 0.9091\n","Epoch 123/200\n","1/1 - 0s - loss: 0.5549 - accuracy: 0.9091\n","Epoch 124/200\n","1/1 - 0s - loss: 0.5438 - accuracy: 0.9091\n","Epoch 125/200\n","1/1 - 0s - loss: 0.5329 - accuracy: 0.9091\n","Epoch 126/200\n","1/1 - 0s - loss: 0.5222 - accuracy: 0.9091\n","Epoch 127/200\n","1/1 - 0s - loss: 0.5118 - accuracy: 0.9091\n","Epoch 128/200\n","1/1 - 0s - loss: 0.5015 - accuracy: 0.9091\n","Epoch 129/200\n","1/1 - 0s - loss: 0.4915 - accuracy: 0.9091\n","Epoch 130/200\n","1/1 - 0s - loss: 0.4817 - accuracy: 0.9091\n","Epoch 131/200\n","1/1 - 0s - loss: 0.4721 - accuracy: 0.9091\n","Epoch 132/200\n","1/1 - 0s - loss: 0.4626 - accuracy: 0.9091\n","Epoch 133/200\n","1/1 - 0s - loss: 0.4534 - accuracy: 0.9091\n","Epoch 134/200\n","1/1 - 0s - loss: 0.4444 - accuracy: 0.9091\n","Epoch 135/200\n","1/1 - 0s - loss: 0.4356 - accuracy: 0.9091\n","Epoch 136/200\n","1/1 - 0s - loss: 0.4270 - accuracy: 0.9091\n","Epoch 137/200\n","1/1 - 0s - loss: 0.4186 - accuracy: 0.9091\n","Epoch 138/200\n","1/1 - 0s - loss: 0.4103 - accuracy: 0.9091\n","Epoch 139/200\n","1/1 - 0s - loss: 0.4023 - accuracy: 0.9091\n","Epoch 140/200\n","1/1 - 0s - loss: 0.3944 - accuracy: 0.9091\n","Epoch 141/200\n","1/1 - 0s - loss: 0.3867 - accuracy: 0.9091\n","Epoch 142/200\n","1/1 - 0s - loss: 0.3792 - accuracy: 0.9091\n","Epoch 143/200\n","1/1 - 0s - loss: 0.3718 - accuracy: 0.9091\n","Epoch 144/200\n","1/1 - 0s - loss: 0.3647 - accuracy: 0.9091\n","Epoch 145/200\n","1/1 - 0s - loss: 0.3576 - accuracy: 0.9091\n","Epoch 146/200\n","1/1 - 0s - loss: 0.3508 - accuracy: 0.9091\n","Epoch 147/200\n","1/1 - 0s - loss: 0.3441 - accuracy: 0.9091\n","Epoch 148/200\n","1/1 - 0s - loss: 0.3375 - accuracy: 0.9091\n","Epoch 149/200\n","1/1 - 0s - loss: 0.3312 - accuracy: 0.9091\n","Epoch 150/200\n","1/1 - 0s - loss: 0.3249 - accuracy: 0.9091\n","Epoch 151/200\n","1/1 - 0s - loss: 0.3188 - accuracy: 0.9091\n","Epoch 152/200\n","1/1 - 0s - loss: 0.3128 - accuracy: 0.9091\n","Epoch 153/200\n","1/1 - 0s - loss: 0.3070 - accuracy: 0.9091\n","Epoch 154/200\n","1/1 - 0s - loss: 0.3013 - accuracy: 0.9091\n","Epoch 155/200\n","1/1 - 0s - loss: 0.2957 - accuracy: 0.9091\n","Epoch 156/200\n","1/1 - 0s - loss: 0.2903 - accuracy: 0.9091\n","Epoch 157/200\n","1/1 - 0s - loss: 0.2849 - accuracy: 0.9091\n","Epoch 158/200\n","1/1 - 0s - loss: 0.2797 - accuracy: 0.9091\n","Epoch 159/200\n","1/1 - 0s - loss: 0.2746 - accuracy: 0.9091\n","Epoch 160/200\n","1/1 - 0s - loss: 0.2696 - accuracy: 0.9091\n","Epoch 161/200\n","1/1 - 0s - loss: 0.2648 - accuracy: 0.9091\n","Epoch 162/200\n","1/1 - 0s - loss: 0.2600 - accuracy: 0.9091\n","Epoch 163/200\n","1/1 - 0s - loss: 0.2553 - accuracy: 0.9091\n","Epoch 164/200\n","1/1 - 0s - loss: 0.2507 - accuracy: 0.9091\n","Epoch 165/200\n","1/1 - 0s - loss: 0.2463 - accuracy: 0.9091\n","Epoch 166/200\n","1/1 - 0s - loss: 0.2419 - accuracy: 0.9091\n","Epoch 167/200\n","1/1 - 0s - loss: 0.2376 - accuracy: 0.9091\n","Epoch 168/200\n","1/1 - 0s - loss: 0.2334 - accuracy: 0.9091\n","Epoch 169/200\n","1/1 - 0s - loss: 0.2293 - accuracy: 0.9091\n","Epoch 170/200\n","1/1 - 0s - loss: 0.2253 - accuracy: 1.0000\n","Epoch 171/200\n","1/1 - 0s - loss: 0.2213 - accuracy: 1.0000\n","Epoch 172/200\n","1/1 - 0s - loss: 0.2175 - accuracy: 1.0000\n","Epoch 173/200\n","1/1 - 0s - loss: 0.2137 - accuracy: 1.0000\n","Epoch 174/200\n","1/1 - 0s - loss: 0.2100 - accuracy: 1.0000\n","Epoch 175/200\n","1/1 - 0s - loss: 0.2064 - accuracy: 1.0000\n","Epoch 176/200\n","1/1 - 0s - loss: 0.2028 - accuracy: 1.0000\n","Epoch 177/200\n","1/1 - 0s - loss: 0.1993 - accuracy: 1.0000\n","Epoch 178/200\n","1/1 - 0s - loss: 0.1959 - accuracy: 1.0000\n","Epoch 179/200\n","1/1 - 0s - loss: 0.1926 - accuracy: 1.0000\n","Epoch 180/200\n","1/1 - 0s - loss: 0.1893 - accuracy: 1.0000\n","Epoch 181/200\n","1/1 - 0s - loss: 0.1861 - accuracy: 1.0000\n","Epoch 182/200\n","1/1 - 0s - loss: 0.1829 - accuracy: 1.0000\n","Epoch 183/200\n","1/1 - 0s - loss: 0.1798 - accuracy: 1.0000\n","Epoch 184/200\n","1/1 - 0s - loss: 0.1768 - accuracy: 1.0000\n","Epoch 185/200\n","1/1 - 0s - loss: 0.1738 - accuracy: 1.0000\n","Epoch 186/200\n","1/1 - 0s - loss: 0.1709 - accuracy: 1.0000\n","Epoch 187/200\n","1/1 - 0s - loss: 0.1681 - accuracy: 1.0000\n","Epoch 188/200\n","1/1 - 0s - loss: 0.1652 - accuracy: 1.0000\n","Epoch 189/200\n","1/1 - 0s - loss: 0.1625 - accuracy: 1.0000\n","Epoch 190/200\n","1/1 - 0s - loss: 0.1598 - accuracy: 1.0000\n","Epoch 191/200\n","1/1 - 0s - loss: 0.1572 - accuracy: 1.0000\n","Epoch 192/200\n","1/1 - 0s - loss: 0.1546 - accuracy: 1.0000\n","Epoch 193/200\n","1/1 - 0s - loss: 0.1520 - accuracy: 1.0000\n","Epoch 194/200\n","1/1 - 0s - loss: 0.1495 - accuracy: 1.0000\n","Epoch 195/200\n","1/1 - 0s - loss: 0.1471 - accuracy: 1.0000\n","Epoch 196/200\n","1/1 - 0s - loss: 0.1447 - accuracy: 1.0000\n","Epoch 197/200\n","1/1 - 0s - loss: 0.1423 - accuracy: 1.0000\n","Epoch 198/200\n","1/1 - 0s - loss: 0.1400 - accuracy: 1.0000\n","Epoch 199/200\n","1/1 - 0s - loss: 0.1378 - accuracy: 1.0000\n","Epoch 200/200\n","1/1 - 0s - loss: 0.1355 - accuracy: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7efef3d32710>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"aHLafTh_HCK7"},"source":["# 문장 생성 함수 만들기\n","\n","def sentence_generation(model, t, current_word, n):\n","    init_word = current_word\n","    sentence = ''\n","    for _ in range(n):\n","        encoded = t.texts_to_sequences([current_word])[0]   # 현재 단어 정수 인코딩\n","        encoded = pad_sequences([encoded], maxlen = 5, padding = 'pre')   # 패딩\n","        result = model.predict_classes(encoded, verbose = 0) # 입력한 현재 단어 x 에 대해 다음 단어 예측, result에 저장\n","        for word, index in t.word_index.items():\n","            if index == result:\n","                break\n","        current_word = current_word + ' ' + word  # 예측 단어를 현재 단어로 변경\n","        sentence = sentence + ' ' + word  # 문장에 예측 단어를 추가하여 저장\n","    sentence = init_word + sentence\n","    return sentence\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyH6SbuRHB9j","outputId":"399c2007-368f-43a1-bbd9-e227dc68d391"},"source":["print(sentence_generation(model, t, '경마장에', 4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["경마장에 있는 말이 뛰고 있다\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlkwSOFDHB4p","outputId":"dcdbef32-3a7b-4725-c682-1a4c2e3adc96"},"source":["print(sentence_generation(model, t, '그의', 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["그의 말이 법이다\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhkGYr9cMicX","outputId":"cf420e4f-e510-4d5c-a445-269fb3c464ec"},"source":["print(sentence_generation(model, t, '가는', 5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["가는 말이 고와야 오는 말이 곱다\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"84MlaLI-MoXM"},"source":[""],"execution_count":null,"outputs":[]}]}