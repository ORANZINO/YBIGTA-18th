{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14665,
     "status": "ok",
     "timestamp": 1615612736356,
     "user": {
      "displayName": "한지영",
      "photoUrl": "",
      "userId": "12883122289782272805"
     },
     "user_tz": -540
    },
    "id": "Em5Obe3lb2nb",
    "outputId": "d1a1f521-74a0-42bb-a869-e26a9a1227c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (4.5.0)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (1.2.1)\n",
      "Requirement already satisfied: colorama in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from konlpy) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Collecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.3.17-cp37-cp37m-macosx_10_9_x86_64.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/taejinoh/opt/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (4.56.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=d78adb5c43e27174d0ec71454f02d8308595ce33ddce6d0889171659441ab112\n",
      "  Stored in directory: /Users/taejinoh/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.4.5\n",
      "    Uninstalling nltk-3.4.5:\n",
      "      Successfully uninstalled nltk-3.4.5\n",
      "Successfully installed nltk-3.5 regex-2021.3.17\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3994,
     "status": "ok",
     "timestamp": 1615612751678,
     "user": {
      "displayName": "한지영",
      "photoUrl": "",
      "userId": "12883122289782272805"
     },
     "user_tz": -540
    },
    "id": "bq-iq_l6crLy",
    "outputId": "ab618895-54bb-47de-ba5b-d6b8501f11fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taejinoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/taejinoh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnxzeeJyb0ym"
   },
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "og5Bc1Cdb0yr"
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4NLNJfbwb0ys"
   },
   "outputs": [],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mUBEiYcdb0ys",
    "outputId": "9806429b-461c-49d4-da3c-07077f726666"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('b', 'c')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2tOhk_sPb0yt",
    "outputId": "9f94d272-4045-49dc-9223-8e9157a9110f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c', 'd'), ('c', 'd', 'c'), ('d', 'c', 'e'), ('c', 'e', 'f')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text[1], n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki-kTq14b0yu"
   },
   "source": [
    "### 문장의 시작과 끝에 위치한 단어를 알 수 있도록 padding을 할 수 있다\n",
    "- Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OviGtxFbb0yu",
    "outputId": "581a4465-3054-4f24-e7dd-f8553bf2055f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpL70QLMb0yu",
    "outputId": "bb4d3356-bc2d-44b5-b0b2-b6a2d4976b49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "list(ngrams(padded_sent, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPNRJoXrb0yv"
   },
   "source": [
    "- Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mea9uSksb0yv",
    "outputId": "7fbf32b6-e66f-4f5d-b02a-de8bb6dd2d99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>']"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhxSf2Snb0yv",
    "outputId": "dac5d80f-c923-480e-960d-9800180785f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'a'),\n",
       " ('<s>', 'a', 'b'),\n",
       " ('a', 'b', 'c'),\n",
       " ('b', 'c', '</s>'),\n",
       " ('c', '</s>', '</s>')]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=3))\n",
    "list(ngrams(padded_sent, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAVOviPGb0yw"
   },
   "source": [
    "- `pad_both_ends`를 이용해 한번에 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPYEOudVb0yw",
    "outputId": "27b50807-ea19-437f-e64c-de1cb3d238a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(bigrams(pad_both_ends(text[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_Dkh_VFb0yw"
   },
   "source": [
    "- `everygrams`를 이용해 한번에 여러 n-gram을 생성할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJtwVgeUb0yw",
    "outputId": "5f99675d-332f-4690-eb4c-3e01e54e4120"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>',),\n",
       " ('a',),\n",
       " ('c',),\n",
       " ('d',),\n",
       " ('c',),\n",
       " ('e',),\n",
       " ('f',),\n",
       " ('</s>',),\n",
       " ('</s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', 'a'),\n",
       " ('a', 'c'),\n",
       " ('c', 'd'),\n",
       " ('d', 'c'),\n",
       " ('c', 'e'),\n",
       " ('e', 'f'),\n",
       " ('f', '</s>'),\n",
       " ('</s>', '</s>'),\n",
       " ('<s>', '<s>', 'a'),\n",
       " ('<s>', 'a', 'c'),\n",
       " ('a', 'c', 'd'),\n",
       " ('c', 'd', 'c'),\n",
       " ('d', 'c', 'e'),\n",
       " ('c', 'e', 'f'),\n",
       " ('e', 'f', '</s>'),\n",
       " ('f', '</s>', '</s>')]"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "padded_bigrams = list(pad_both_ends(text[1], n=3))\n",
    "list(everygrams(padded_bigrams, max_len=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t24dxe8b0yx"
   },
   "source": [
    "**학습을 위해서는 코퍼스의 모든 단어를 포함하는 단어사전(Vocabulary)이 필요**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FW_95NTWb0yx",
    "outputId": "90066ccc-dad1-40a5-c1d1-c7f3970fe4f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjVR6xcQb0yx"
   },
   "source": [
    "`padded_everygram_pipeline`으로 모든 과정 한번에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nHE7vzib0yx"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KS8HISiGb0yx",
    "outputId": "552ba017-e322-4aef-c465-4dd8f6f46544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('a',), ('b',), ('c',), ('</s>',), ('<s>', 'a'), ('a', 'b'), ('b', 'c'), ('c', '</s>')]\n",
      "\n",
      "[('<s>',), ('a',), ('c',), ('d',), ('c',), ('e',), ('f',), ('</s>',), ('<s>', 'a'), ('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f'), ('f', '</s>')]\n",
      "\n",
      "#############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'b', 'c', '</s>', '<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ngramlize_sent in train:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('#############')\n",
    "list(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5pfPsd_b0yx"
   },
   "source": [
    "## 실제 데이터로 N-Gram 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NiJIekLb0yy"
   },
   "source": [
    "### 데이터 가져오기 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEronpCGb0yy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "text = requests.get(url).content.decode('utf8')\n",
    "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "    fout.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1309,
     "status": "ok",
     "timestamp": 1615384881883,
     "user": {
      "displayName": "Jinwook Huh",
      "photoUrl": "",
      "userId": "13981448195995220893"
     },
     "user_tz": -540
    },
    "id": "kQGRIME4b0yy",
    "outputId": "59358993-b3b2-4145-ccbd-58de19afcd5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1615384887776,
     "user": {
      "displayName": "Jinwook Huh",
      "photoUrl": "",
      "userId": "13981448195995220893"
     },
     "user_tz": -540
    },
    "id": "6SF1K9XBb0yy",
    "outputId": "85bb4f8d-4f33-4f40-a30c-eba20fd1f520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish \n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1615384890091,
     "user": {
      "displayName": "Jinwook Huh",
      "photoUrl": "",
      "userId": "13981448195995220893"
     },
     "user_tz": -540
    },
    "id": "229QAFgBb0yy",
    "outputId": "0e24b15a-d6e5-4f75-d272-be51cd67200c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgxZTpcCb0yz"
   },
   "outputs": [],
   "source": [
    "# 3-grams 언어모델 생성\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0AzYgDcb0yz"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03dg0dtub0yz"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt5ARhTlb0yz",
    "outputId": "74ae221a-ad7e-498f-876f-04b64a7d02cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2n7xITBb0yz",
    "outputId": "6f5a12d0-c687-4aca-ed56-2fbd29d593ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1391"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXYAusY2b0yz",
    "outputId": "c979233a-73e4-4cd8-d080-f9fa2711710b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSHi4Wx_b0y0",
    "outputId": "41d87686-ea46-4654-af43-9f2bda1bf1d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('language', 'is', 'never', 'random', '<UNK>', '.')\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup('language is never random lah .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1NeSF9xb0y0"
   },
   "source": [
    "## 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqBaHZXmb0y0",
    "outputId": "abe2c20e-15ad-4c4d-a61d-af7f5a4223ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 19611 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOj2ZkCVb0y0",
    "outputId": "9f771492-e47a-4f03-c50b-6cc67847833d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts['language'] # i.e. Count('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSZclSghb0y0",
    "outputId": "2b5e4aa7-59d3-4bc7-f1a7-700c59aeaa89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language']]['is'] # i.e. Count('is'|'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc0tP1ZJb0y1",
    "outputId": "1bc4d339-8dfe-4bbc-890d-e182dbc5c86d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xio_u7Owb0y1",
    "outputId": "0ca8c4e0-eb11-4388-b226-6389f0e19cc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003691671588895452"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('language') # P('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DycTuf1Ib0y1",
    "outputId": "3c2fe467-76cc-4c07-efda-3239278a3c0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('is', 'language'.split())  # P('is'|'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdvDwrShb0y1",
    "outputId": "4e1f2c24-bc18-4631-9956-2ea3ade7c380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('never', 'language is'.split())  # P('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "augWli3Gb0y1"
   },
   "source": [
    "## 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1WVLLE5b0y1",
    "outputId": "d571667a-6dde-487b-af9e-db422d7b6c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'carroll', 'used', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'a', 'half', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZw_Hzreb0y2"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10jm44oXb0y2",
    "outputId": "1bfb6bcb-656d-4a89-db5d-047987a719cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and carroll used hypothesis testing has been used, and a half.'"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 20, random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ0QYfaFb0y2",
    "outputId": "c68ac8c4-d2e5-49f8-d071-3dc1a62cb15e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'situation up symmetrically, a measure of salience, whereas in fact they were merely testing whether they had been selected at random, and the random in'"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 28, random_seed=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNLqTebSb0y2"
   },
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW0l5efnb0y2"
   },
   "source": [
    "## Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJHpTFNmb0y2"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re  \n",
    "okt=Okt() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y8ZUT9Pb0y3"
   },
   "outputs": [],
   "source": [
    "text1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'\n",
    "text2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "text3 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6698,
     "status": "ok",
     "timestamp": 1615384917476,
     "user": {
      "displayName": "Jinwook Huh",
      "photoUrl": "",
      "userId": "13981448195995220893"
     },
     "user_tz": -540
    },
    "id": "5LB0cnOqb0y3",
    "outputId": "ae88aac6-d4e5-40fd-d6f5-13d5cb699338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['정부', '가', '발표', '하는', '물가상승률', '과', '소비자', '가', '느끼는', '물가상승률', '은', '다르다']\n"
     ]
    }
   ],
   "source": [
    "token=re.sub(\"(\\.)\",\"\",text1)  # 정규 표현식을 통해 온점을 제거 \n",
    "token=okt.morphs(token)  # OKT 형태소 분석기를 통해 토큰화 \n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Isxo2rdHb0y3"
   },
   "outputs": [],
   "source": [
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "    if voca not in word2index.keys():  \n",
    "        word2index[voca]=len(word2index)  # word2index에 없는 토큰은 추가   \n",
    "        bow.insert(len(word2index)-1,1) \n",
    "    else:\n",
    "        index=word2index.get(voca)  # 토큰의 인덱스 가져오기\n",
    "        bow[index]=bow[index]+1  # 빈도수 += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXq5i6krb0y3",
    "outputId": "05f010b4-bf48-4151-8870-a89f61e8e860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "print(word2index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A7WT5Cxb0y3",
    "outputId": "3736ba2a-54ff-4771-8b6b-d325febfb7dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ghn0Btfbb0y4",
    "outputId": "8591923b-c84a-467f-ef0e-878a04866448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "token=re.sub(\"(\\.)\",\"\",text2)\n",
    "token=okt.morphs(token)\n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "    if voca not in word2index.keys():  \n",
    "        word2index[voca]=len(word2index)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        index=word2index.get(voca)\n",
    "        bow[index]=bow[index]+1\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJkZi7Aib0y4",
    "outputId": "d4472742-6572-47b3-8b1b-00cb420c7094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "token=re.sub(\"(\\.)\",\"\",text3)\n",
    "token=okt.morphs(token)\n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "    if voca not in word2index.keys():  \n",
    "        word2index[voca]=len(word2index)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        index=word2index.get(voca)\n",
    "        bow[index]=bow[index]+1\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pzov4T-b0y4"
   },
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMRzgGyTb0y4",
    "outputId": "5957977f-0449-41d9-8223-e793800fbb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 각 단어의 빈도수\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lT3PpEOb0y4"
   },
   "source": [
    "## 불용어 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCUZjrOQb0y4"
   },
   "source": [
    "### 직접 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Pz225xDb0y5",
    "outputId": "20e54cd4-ac39-4e35-f12b-000fa730e204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "copSFgCgb0y5"
   },
   "source": [
    "### CountVectorizer 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sps-yZ92b0y5",
    "outputId": "b21f6f69-8578-44ba-98a2-29f5b9562cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qzI5t8ub0y5"
   },
   "source": [
    "### NLTK 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1615386106164,
     "user": {
      "displayName": "Jinwook Huh",
      "photoUrl": "",
      "userId": "13981448195995220893"
     },
     "user_tz": -540
    },
    "id": "Qy8UO4b3b0y6",
    "outputId": "224639ea-7ec5-4528-9249-b039e4e190ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDbWMKu2b0y6"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsMd6iNcb0y6"
   },
   "source": [
    "## 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9GhMo5gb0y6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-auiEU14b0y6"
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV8dmrXzb0y7"
   },
   "outputs": [],
   "source": [
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU1IPENMb0y7"
   },
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_RpkogMb0y7",
    "outputId": "380bf3e3-d0e0-4b2e-a0b5-52bdb8eb19d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]        \n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGv4_YW_b0y7"
   },
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuxM03EWb0y7",
    "outputId": "77f560c4-34dd-41e9-9895-edc5cc8e9334"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkRdmALCb0y7"
   },
   "source": [
    "### 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4DiZ-pUb0y8",
    "outputId": "d6a76b47-0393-4a3b-e165-8a1a065fc65e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52qLieH1b0y8"
   },
   "source": [
    "## Scikit-Learn을 이용한 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAUCyaHsb0y8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlwZoM2sb0y8",
    "outputId": "497fa858-9383-4beb-c934-f62d7a0665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93tKWAixb0y8"
   },
   "source": [
    "# Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbg_RJFMb0y8"
   },
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz-ICbWLb0y8"
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jU6Pc6wtb0y9"
   },
   "outputs": [],
   "source": [
    "doc1=np.array([0,1,1,1])\n",
    "doc2=np.array([1,0,1,1])\n",
    "doc3=np.array([2,0,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAUqZ6Tpb0y9",
    "outputId": "a830701e-2d4b-4ebb-ed6c-0869704f436b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6666666666666667\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(doc1, doc2)) #문서1/문서2\n",
    "print(cos_sim(doc1, doc3)) #문서1/문서3\n",
    "print(cos_sim(doc2, doc3)) #문서2/문서3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzedTUSIb0y9"
   },
   "source": [
    "## Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGpu2mLYb0y9",
    "outputId": "1b677790-df08-4932-fa15-c1da17182db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']\n",
      "['apple', 'banana', 'coupon', 'passport', 'love', 'you']\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"apple banana everyone like likey watch card holder\"\n",
    "doc2 = \"apple banana coupon passport love you\"\n",
    "\n",
    "# 토큰화\n",
    "tokenized_doc1 = doc1.split()\n",
    "tokenized_doc2 = doc2.split()\n",
    "\n",
    "print(tokenized_doc1)\n",
    "print(tokenized_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECcFG54gb0y9",
    "outputId": "b3ef6e99-9364-4f09-84c5-b661c38e0a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you', 'coupon', 'apple', 'passport', 'like', 'holder', 'likey', 'love', 'banana', 'everyone', 'card', 'watch'}\n"
     ]
    }
   ],
   "source": [
    "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
    "print(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSD-YG2vb0y9",
    "outputId": "0d74e1ac-2539-441e-f870-9965b1d29aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banana', 'apple'}\n"
     ]
    }
   ],
   "source": [
    "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEgkhoyRb0y-",
    "outputId": "bb098acf-545b-4ca6-89e7-53bccc810f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(len(intersection)/len(union))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0YHYQaob0y-"
   },
   "source": [
    "## Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGQmdXbxb0y-",
    "outputId": "3ee40f04-7c77-4df8-9212-8b541cf36702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "3.1622776601683795\n",
      "2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print(dist(doc1,docQ))\n",
    "print(dist(doc2,docQ))\n",
    "print(dist(doc3,docQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykxBZ1kVb0y-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x1NeSF9xb0y0",
    "augWli3Gb0y1",
    "6Pzov4T-b0y4"
   ],
   "name": "실습.ipynb",
   "provenance": [
    {
     "file_id": "1ig67S-nQ9G5sOcLUZAJlkVfyL2Ufcl9z",
     "timestamp": 1615437512439
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
